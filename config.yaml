---
# config

common:
  data_type : iid #data_type = data distribution one among ['iid','one-class-niid','one-class-niid-majority','two-class-niid','dirichlet-niid']
  hpo : false #hpo = Hyperparameter optimisation true or false
  dataset : shakespeare #data_set = data set used  one among [fashionmnist,mnist,cifar-10,shakespeare ]
  dirichlet_alpha : 0.11 #dirichlet concentration parameter
  target_acc : 0.80
  model : lstm-shakespeare #one among [mobilenetv2, simplecnn, simplednn, kerasexpcnn, mnistcnn,efficientnet,fedavgcnn,fmcnn,resnet-18,lstm-shakespeare]
  optimizer : sgd # one among [sgd,adam]
  simulation : true

server:
  max_rounds : 11
  address : 127.0.0.1
  fraction_fit : 0.05
  min_fit_clients: 2
  min_avalaible_clients : 200  # total number of clients participating in training 
  fraction_evaluate : 0.025
  strategy : fedavg #Strategy : choices=['fedavg', 'fedyogi', 'fedadagrad', 'fedavgm','fedprox']

client:
  epochs : 1
  batch_size : 64
  lr: 0.01  #[0.0001,0.001,0.005,0.01,0.1,0.2]
  save_train_res : True

fedex:
  hyperparam_config_nr : 120 # size of hyperparameter search space
  hyperparam_file : './hyperparam-logs/indices.json'

shakespeare:
  sequence_length : 100 # the length of input sequences characters
  vocab_size : 78
  train_file: /home/mak36/Desktop/curr_work/FedAdap_v2/shakespare_train_processed.pkl
  test_file: /home/mak36/Desktop/curr_work/FedAdap_v2/shakespare_test_processed.pkl




